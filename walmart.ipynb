{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import sklearn.cross_validation, sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import scipy.stats\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "pl.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "column_dtypes = {'ScanCount': 'int8'}\n",
    "\n",
    "train = pd.read_csv('train.csv', low_memory=False, dtype=column_dtypes)\n",
    "test = pd.read_csv('test.csv', low_memory=False, dtype=column_dtypes)\n",
    "types = np.unique(train.TripType.values)\n",
    "types_dictionary = {t: i for i, t in  enumerate(types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_Upc(data):\n",
    "    data['UpcPrefix'] = data.Upc.map(lambda x: x // 10**10 % 10 ).astype('int8')\n",
    "    data['UpcManufacturer'] = data.Upc.map(lambda x: (x // 10**5) - (x // 10**10)*10**5 ).astype('int32')\n",
    "    data['UpcProduct'] = data.Upc.map(lambda x: x % 10**5).astype('int32')\n",
    "    data.ix[data.Upc == -1, ['UpcPrefix', 'UpcManufacturer', 'UpcProduct']] = -1\n",
    "    del data['Upc']\n",
    "    return data\n",
    "\n",
    "def preprocess_columns(data):\n",
    "    data.Upc = data.Upc.fillna(-1)\n",
    "    data.FinelineNumber = data.FinelineNumber.fillna(-1)\n",
    "    data = transform_Upc(data)\n",
    "    data.FinelineNumber = data.FinelineNumber.astype('int16')\n",
    "    return data\n",
    "\n",
    "train = preprocess_columns(train)\n",
    "test = preprocess_columns(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequence_feature(feature):\n",
    "    dict_counts = dict(feature.value_counts())\n",
    "    series = feature.map(lambda x: dict_counts[x])\n",
    "    return (series - series.min())/(series.max() - series.min())\n",
    "\n",
    "def encode_features(data):\n",
    "    data['FinelineNumberFreq'] = get_frequence_feature(data.FinelineNumber)\n",
    "    data['UpcManufacturerFreq'] = get_frequence_feature(data.UpcManufacturer)\n",
    "    data['UpcProductFreq'] = get_frequence_feature(data.UpcProduct)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = train.append(test)\n",
    "data = encode_features(data)\n",
    "train = data[:len(train)]\n",
    "test = data[len(train):]\n",
    "\n",
    "del data, test['TripType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def at(l):\n",
    "    return l.iloc[0]\n",
    "def mklist(l):\n",
    "    return list(l)\n",
    "\n",
    "train_grouped = train.groupby('VisitNumber').agg({'TripType': at, 'Weekday': at, 'ScanCount': mklist, \n",
    "                                                  'DepartmentDescription': at, 'FinelineNumberFreq': mklist,\n",
    "                                                  'UpcPrefix': mklist, 'UpcManufacturerFreq': mklist,\n",
    "                                                  'UpcProductFreq': mklist}).reset_index()\n",
    "test_grouped = test.groupby('VisitNumber').agg({'Weekday': at, 'ScanCount': mklist, 'DepartmentDescription': at, \n",
    "                                                'FinelineNumberFreq': mklist, 'UpcPrefix': mklist,\n",
    "                                                'UpcManufacturerFreq': mklist, 'UpcProductFreq': mklist}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_grouped_features(data):\n",
    "    \n",
    "    data['LogNumberOfReturned'] = data.ScanCount.map(lambda x: np.log(-sum([a for a in x if a < 0]) + 1))\n",
    "    data['NumberOfPurchased'] = data.ScanCount.map(lambda x: sum([a for a in x if a > 0]))\n",
    "    \n",
    "    data['MaxScanCount'] = data.ScanCount.map(lambda x: np.max(x)).astype('int8')\n",
    "    data['MinScanCount'] = data.ScanCount.map(lambda x: np.min(x)).astype('int8')\n",
    "    data['MaxMinScanCount'] = data.ScanCount.map(lambda x: np.max(x) - np.min(x)).astype('int16')\n",
    "    data['MeanScanCount'] = data.ScanCount.map(lambda x: np.mean(x))\n",
    "    \n",
    "    data['ModeUpcPrefix'] = data.UpcPrefix.map(lambda x: scipy.stats.mode(x)[0][0]).astype('int8')\n",
    "    #need more upcPrefix\n",
    "    \n",
    "    data['ModeFinelineNumberFreq'] = data.FinelineNumberFreq.map(lambda x: scipy.stats.mode(x)[0][0])\n",
    "    data['MeanFinelineNumberFreq'] = data.FinelineNumberFreq.map(lambda x: np.mean(x))\n",
    "    data['MaxFinelineNumberFreq'] = data.FinelineNumberFreq.map(lambda x: np.max(x))\n",
    "    \n",
    "    data['ModeUpcManufacturerFreq'] = data.UpcManufacturerFreq.map(lambda x: scipy.stats.mode(x)[0][0])\n",
    "    data['MeanUpcManufacturerFreq'] = data.UpcManufacturerFreq.map(lambda x: np.mean(x))\n",
    "    data['MaxUpcManufacturerFreq'] = data.UpcManufacturerFreq.map(lambda x: np.max(x))\n",
    "    \n",
    "    \n",
    "    data['ModeUpcProductFreq'] = data.UpcProductFreq.map(lambda x: scipy.stats.mode(x)[0][0])\n",
    "    data['MeanUpcProductFreq'] = data.UpcProductFreq.map(lambda x: np.mean(x))\n",
    "    data['MaxUpcProductFreq'] = data.UpcProductFreq.map(lambda x: np.max(x))\n",
    "    \n",
    "    data['SumScanCount*FlNFreq'] = data.apply(lambda r: sum(np.array(r.FinelineNumberFreq)*np.array(r.ScanCount)), axis=1)\n",
    "    data['SumScanCount*UPtFreq'] = data.apply(lambda r: sum(np.array(r.UpcProductFreq)*np.array(r.ScanCount)), axis=1)\n",
    "    data['SumScanCount*UMrFreq'] = data.apply(lambda r: sum(np.array(r.UpcManufacturerFreq)*np.array(r.ScanCount)), axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_grouped = construct_grouped_features(train_grouped)\n",
    "test_grouped = construct_grouped_features(test_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sns.countplot(x='TripType', hue='Weekday', data=train_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dummy_features(data, dummy_columns):\n",
    "    data = pd.get_dummies(data, sparse=True, dummy_na=False, columns=dummy_columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 features constructed\n",
      "Memory usage of dataframe train_dummies is 15.69 Mb\n",
      "Memory usage of dataframe test_dummies is 14.96 Mb\n"
     ]
    }
   ],
   "source": [
    "expanded_features = ['UpcPrefix', 'UpcManufacturerFreq', 'UpcProductFreq', 'FinelineNumberFreq', 'ScanCount']\n",
    "\n",
    "data = train_grouped.append(test_grouped)\n",
    "data = data[[col for col in data.columns if col not in expanded_features]]\n",
    "\n",
    "data = construct_dummy_features(data, ['Weekday', 'DepartmentDescription', 'ModeUpcPrefix'])\n",
    "\n",
    "train_dummies = data.iloc[:len(train_grouped)]\n",
    "test_dummies = data.iloc[len(train_grouped):]\n",
    "del data, test_dummies['TripType']\n",
    "print(len(test_dummies.columns), 'features constructed')\n",
    "print('Memory usage of dataframe train_dummies is %3.2f Mb' % (train_dummies.memory_usage(index=True).sum()/(1024*1024)))\n",
    "print('Memory usage of dataframe test_dummies is %3.2f Mb' % (test_dummies.memory_usage(index=True).sum()/(1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del train, test, train_grouped, test_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prior_probabilities(data):\n",
    "    prior_probabilities = np.zeros(len(types))\n",
    "    prior_probabilities += np.array([len(data[data.TripType == trip_type]) for trip_type in types])\n",
    "    prior_probabilities /= prior_probabilities.sum()\n",
    "    return prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict(train_array, train_labels, test_array, prior_probabilities=None, algorithm='rf', alpha=None, \n",
    "                plot_importance=True, feature_names=None):\n",
    "    if algorithm == 'xgb':\n",
    "        with open('xgb.fmap', 'w') as outfile:\n",
    "            for i, feature_name in enumerate(feature_names):\n",
    "                outfile.write('{0}\\t{1}\\tq\\n'.format(i, feature_name))\n",
    "\n",
    "        clf = xgb.XGBClassifier(n_estimators=30, max_depth=8, learning_rate=0.4, objective='multi:softprob', \n",
    "                                colsample_bytree=0.45, subsample=0.5, )\n",
    "        clf.fit(train_array, train_labels)\n",
    "        prediction_matrix = clf.predict_proba(test_array)\n",
    "        if plot_importance and feature_names:\n",
    "            importances = clf.booster().get_fscore('xgb.fmap')\n",
    "            feature_names = list(importances.keys())\n",
    "            importances = np.array(list(importances.values()))\n",
    "            pl.title('Feature Importance')\n",
    "            sorted_indices = np.argsort(importances)[::-1]\n",
    "            for i, k in enumerate(sorted_indices):\n",
    "                print('%2d (feature %2d):' % (i, k), feature_names[k], 'Value = %.5f' % importances[k])\n",
    "#             print([feature_names[k] for k in sorted_indices])\n",
    "            sns.barplot(np.arange(len(feature_names)), importances[sorted_indices], color='r')\n",
    "                        \n",
    "    else:\n",
    "        if algorithm == 'rf':\n",
    "            clf = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=1, max_features=0.2, n_jobs=-1)\n",
    "        clf.fit(train_array, train_labels)\n",
    "        prediction_matrix = clf.predict_proba(test_array)\n",
    "        if plot_importance and feature_names:\n",
    "            importances = clf.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "            pl.title('Feature Importance')\n",
    "            sorted_indices = np.argsort(importances)[::-1]\n",
    "            for i, k in enumerate(sorted_indices):\n",
    "                print('%2d (feature %2d):' % (i, k), feature_names[k], 'Value = %.5f' % importances[k])\n",
    "#             print([feature_names[k] for k in sorted_indices])\n",
    "\n",
    "            pl.bar(range(len(feature_names)), importances[sorted_indices], color='r',\n",
    "                   yerr=std[sorted_indices], align='center')\n",
    "            pl.xticks(range(len(feature_names)), sorted_indices)\n",
    "            pl.xlim([-1, len(feature_names) + 1])\n",
    "            \n",
    "    if not prior_probabilities is None:\n",
    "        coeff = 0.1\n",
    "        prediction_matrix = (prediction_matrix + [prior_probabilities*coeff]*len(prediction_matrix))/(1. + coeff)\n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_score(train_dummies, grid_space=[0], algorithm='rf', plot_importance=True):\n",
    "    if len(grid_space) > 1:\n",
    "        alphas, scores = [], []\n",
    "        min_alpha, min_score = 1., 10**5\n",
    "    \n",
    "    for alpha in grid_space:\n",
    "        if len(grid_space) > 1:\n",
    "            print('alpha =', alpha)\n",
    "        y = train_dummies.TripType.values\n",
    "        for train_indices, test_indices in sklearn.cross_validation.StratifiedShuffleSplit(y, n_iter=1, test_size=0.1):\n",
    "            data_train = train_dummies.iloc[train_indices]\n",
    "            data_test = train_dummies.iloc[test_indices]\n",
    "\n",
    "    #         prior_probabilities = get_prior_probabilities(data_train)\n",
    "            feature_names = [col for col in data_train.columns if col not in ['TripType']]\n",
    "            prediction_matrix = fit_predict(data_train[feature_names].values, \n",
    "                                            data_train.TripType.values, \n",
    "                                            data_test[[col for col in data_test.columns \n",
    "                                                       if col not in ['TripType']]].values, \n",
    "                                            algorithm=algorithm, alpha=alpha, plot_importance=plot_importance, \n",
    "                                            feature_names=feature_names)\n",
    "\n",
    "            prediction_matrix = np.delete(prediction_matrix, 7, axis=1)\n",
    "            y_true = data_test.TripType.values\n",
    "            \n",
    "            mlogloss = sklearn.metrics.log_loss(y_true, prediction_matrix) \n",
    "            \n",
    "            print('mlogloss =', mlogloss, end='\\n\\n')\n",
    "            \n",
    "            if len(grid_space) > 1:\n",
    "                alphas.append(alpha)\n",
    "                scores.append(mlogloss)\n",
    "                if mlogloss <= min_score:\n",
    "                    min_score = mlogloss\n",
    "                    min_alpha = alpha\n",
    "    if len(grid_space) > 1:\n",
    "        return alphas, scores, min_alpha, min_score\n",
    "    else:\n",
    "        return mlogloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithms = ['rf', 'xgb']\n",
    "algorithm = 'xgb'\n",
    "\n",
    "validation = True\n",
    "\n",
    "if validation:\n",
    "    \n",
    "    compute_optimal = False\n",
    "    if compute_optimal:\n",
    "        grid_space = np.linspace(0.1, 0.9, 10)\n",
    "        alphas, scores, min_alpha, min_score = train_and_score(train_dummies, grid_space=grid_space,\n",
    "                                                               algorithm=algorithm, plot_importance=False)\n",
    "        print('MIN ALPHA =', min_alpha, 'MIN_MLOGLOSS =', min_score)\n",
    "        pl.plot(alphas, scores, 'r.--')\n",
    "    else:\n",
    "        mlogloss = train_and_score(train_dummies, algorithm=algorithm)\n",
    "    \n",
    "elif not validation:\n",
    "#     prior_probabilities = get_prior_probabilities(train)\n",
    "    prediction_matrix = fit_predict(train_dummies[[col for col in train_dummies.columns \n",
    "                                                   if col not in ['TripType']]].values, \n",
    "                                    train_dummies.TripType.values, test_dummies.values, algorithm=algorithm)\n",
    "    \n",
    "    prediction_df = pd.DataFrame(prediction_matrix, columns=['TripType_' + str(i) for i in types])\n",
    "    prediction_df['VisitNumber'] = test_dummies.VisitNumber.values\n",
    "    \n",
    "    prediction_df = prediction_df[['VisitNumber'] + [x for x in prediction_df.columns if x != 'VisitNumber']]\n",
    "    del prediction_matrix\n",
    "    \n",
    "    filename = 'prediction.csv'\n",
    "    prediction_df.to_csv(filename, index=False)\n",
    "    print(filename, 'was created')\n",
    "    del prediction_df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
