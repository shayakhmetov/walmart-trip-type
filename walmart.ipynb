{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some UPC -> 341776144100, 7410811099, 25541500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import sklearn.cross_validation, sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "pl.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "column_dtypes = {'ScanCount': 'int8'}\n",
    "\n",
    "train = pd.read_csv('train.csv', low_memory=False, dtype=column_dtypes)\n",
    "test = pd.read_csv('test.csv', low_memory=False, dtype=column_dtypes)\n",
    "types = np.unique(train.TripType.values)\n",
    "types_dictionary = {t: i for i, t in  enumerate(types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_Upc_len12(upc): \n",
    "    if upc // 10**11 % 10 != 0:\n",
    "        return upc // 10\n",
    "    else:\n",
    "        return upc\n",
    "\n",
    "def transform_Upc(data):\n",
    "    data.Upc = data.Upc.map(lambda x: transform_Upc_len12(x))\n",
    "    for i in range(1,12):\n",
    "        data['Upc_' + str(i)] = data.Upc // 10**(12 - i - 1) % 10\n",
    "        data['UpcManufacturer'] = (data.Upc // 10**5) - (data.Upc // 10**10)*10**5\n",
    "        data['UpcProduct'] = data.Upc % 10**5\n",
    "    return data\n",
    "\n",
    "def preprocess_columns(data):\n",
    "    data.Upc = data.Upc.fillna(0).astype('int64')\n",
    "    data.FinelineNumber = data.FinelineNumber.fillna(10000).astype('int16')\n",
    "    data.DepartmentDescription = data.DepartmentDescription.fillna('UNKNOWN')\n",
    "    data = transform_Upc(data)\n",
    "    return data\n",
    "\n",
    "train = preprocess_columns(train)\n",
    "test = preprocess_columns(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_checksum_UPC(upc):\n",
    "    assert len(upc) == 11\n",
    "    res = sum([int(digit) for i, digit in enumerate(upc) if i % 2 == 0])*3\n",
    "    res += sum([int(digit) for i, digit in enumerate(upc) if i % 2 == 1])\n",
    "    res = 10 - (res % 10)\n",
    "    if res == 10:\n",
    "        res = 0\n",
    "    return res\n",
    "    \n",
    "def checksum_UPC(upc):\n",
    "    assert len(upc) == 12\n",
    "    res = compute_checksum_UPC(upc[:-1])\n",
    "    return res == int(upc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequence_feature(feature):\n",
    "    dict_counts = dict(feature.value_counts())\n",
    "    series = feature.map(lambda x: dict_counts[x])\n",
    "    return (series - series.min())/(series.max() - series.min())\n",
    "\n",
    "def most_freq_dummies(feature, feature_freq, threshold=0.2):\n",
    "    mask = feature_freq >= threshold\n",
    "    to_dummies = np.unique((feature + 1).multiply(mask).values)\n",
    "    return (to_dummies - 1)[1:]\n",
    "    \n",
    "\n",
    "def encode_features(data):\n",
    "    to_dummies = {}\n",
    "    data['FinelineNumberFreq'] = get_frequence_feature(data.FinelineNumber)\n",
    "    to_dummies['FinelineNumber'] = most_freq_dummies(data.FinelineNumber, data.FinelineNumberFreq, threshold=0.2)\n",
    "    \n",
    "    data['UpcManufacturerFreq'] = get_frequence_feature(data.UpcManufacturer)\n",
    "    to_dummies['UpcManufacturer'] = most_freq_dummies(data.UpcManufacturer, data.UpcManufacturerFreq, threshold=0.1)\n",
    "    \n",
    "    data['UpcProductFreq'] = get_frequence_feature(data.UpcProduct)\n",
    "    to_dummies['UpcProduct'] = most_freq_dummies(data.UpcProduct, data.UpcProductFreq, threshold=0.05)\n",
    "    \n",
    "    data['DepartmentFreq'] = get_frequence_feature(data.DepartmentDescription)\n",
    "    return data, to_dummies\n",
    "\n",
    "data = train.append(test)\n",
    "data, to_dummies = encode_features(data)\n",
    "train = data[:len(train)]\n",
    "test = data[len(train):]\n",
    "\n",
    "del data, test['TripType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def at(l):\n",
    "    return l.iloc[0]\n",
    "def mklist(l):\n",
    "    return list(l)\n",
    "\n",
    "columns = ['TripType', 'Weekday', 'DepartmentDescription', 'DepartmentFreq']\n",
    "columns_complement = [col for col in train.columns if col not in columns + ['VisitNumber']]\n",
    "agg_dict = {col:at for col in columns}\n",
    "agg_dict.update({col:mklist for col in columns_complement})\n",
    "\n",
    "train_grouped = train.groupby('VisitNumber').agg(agg_dict).reset_index()    \n",
    "agg_dict.pop(\"TripType\", None)\n",
    "test_grouped = test.groupby('VisitNumber').agg(agg_dict).reset_index()\n",
    "\n",
    "for column in columns_complement:\n",
    "    train_grouped[column] = train_grouped[column].map(lambda x: np.array(x))\n",
    "    test_grouped[column] = test_grouped[column].map(lambda x: np.array(x))\n",
    "\n",
    "del agg_dict, columns, columns_complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_grouped_features(data, to_dummies):\n",
    "    \n",
    "    data['LogNumberOfReturned'] = data.ScanCount.map(lambda x: np.log(-sum([a for a in x if a < 0]) + 1))\n",
    "    data['LogNumberOfPurchased'] = data.ScanCount.map(lambda x: np.log(sum([a for a in x if a > 0]) + 1))\n",
    "    data['MedianScanCount'] = data.ScanCount.map(lambda x: np.median(x))\n",
    "    data['MaxScanCount'] = data.ScanCount.map(lambda x: np.max(x))\n",
    "    \n",
    "    for j in range(1,2):\n",
    "        for i in range(10):\n",
    "            data['Upc_' + str(j) + '_value' + str(i)] = data['Upc_' + str(j)].map(lambda x: (i == x).sum()/len(x))\n",
    "    \n",
    "    for elem in to_dummies['FinelineNumber']:\n",
    "        data['FlNDummy_' + str(elem)] = data.FinelineNumber.map(lambda x: (elem == x).sum()/len(x))\n",
    "    for elem in to_dummies['UpcManufacturer']:\n",
    "        data['UMrDummy_' + str(elem)] = data.UpcManufacturer.map(lambda x: (elem == x).sum()/len(x))\n",
    "    for elem in to_dummies['UpcProduct']:\n",
    "        data['UPtDummy_' + str(elem)] = data.UpcProduct.map(lambda x: (elem == x).sum()/len(x))\n",
    "    \n",
    "    data['MedianFinelineNumberFreq'] = data.FinelineNumberFreq.map(lambda x: np.median(x))\n",
    "    data['MedianUpcManufacturerFreq'] = data.UpcManufacturerFreq.map(lambda x: np.median(x))\n",
    "    data['MedianUpcProductFreq'] = data.UpcProductFreq.map(lambda x: np.median(x))\n",
    "    \n",
    "#     data['MedianFinelineNumber'] = data.FinelineNumber.map(lambda x: np.median(x))\n",
    "#     data['MedianUpcManufacturer'] = data.UpcManufacturer.map(lambda x: np.median(x))\n",
    "#     data['MedianUpcProduct'] = data.UpcProduct.map(lambda x: np.median(x))\n",
    "    \n",
    "    data['ModeFinelineNumber'] = data.FinelineNumber.map(lambda x: np.argmax(np.bincount(x)))\n",
    "    data['ModeUpcManufacturer'] = data.UpcManufacturer.map(lambda x: np.argmax(np.bincount(x)))\n",
    "    data['ModeUpcProduct'] = data.UpcProduct.map(lambda x: np.argmax(np.bincount(x)))\n",
    "    \n",
    "    data['StdRelUMrUpt'] = data.apply(lambda r: \n",
    "                                      np.std(np.log(r.UpcManufacturerFreq/(r.UpcProductFreq+1)+1)), axis=1)\n",
    "    data['MeanDiffUMrUpt'] = data.apply(lambda r: \n",
    "                                      np.mean(r.UpcManufacturerFreq - r.UpcProductFreq), axis=1)\n",
    "    \n",
    "    \n",
    "    data['SumSnCtFlNFreq'] = data.apply(lambda r: sum(r.FinelineNumberFreq*r.ScanCount), axis=1)\n",
    "    data['SumSnCtUPtFreq'] = data.apply(lambda r: sum(r.UpcProductFreq*r.ScanCount), axis=1)\n",
    "    data['SumSnCtUMrFreq'] = data.apply(lambda r: sum(r.UpcManufacturerFreq*r.ScanCount), axis=1)\n",
    "    \n",
    "    data['StdSnCtFlNFreq'] = data.apply(lambda r: np.std(r.FinelineNumberFreq*r.ScanCount), axis=1)\n",
    "    data['StdSnCtUPtFreq'] = data.apply(lambda r: np.std(r.UpcProductFreq*r.ScanCount), axis=1)\n",
    "    data['StdSnCtUMrFreq'] = data.apply(lambda r: np.std(r.UpcManufacturerFreq*r.ScanCount), axis=1)\n",
    "    \n",
    "    data['MaxFlNnUptnUMr'] = data.apply(lambda r:\n",
    "                                        np.max(r.UpcProductFreq + r.UpcManufacturerFreq +\n",
    "                                               r.FinelineNumberFreq), axis=1)\n",
    "    data['MedianFlNnUptnUMr'] = data.apply(lambda r:\n",
    "                                           np.median(r.UpcProductFreq + r.UpcManufacturerFreq + \n",
    "                                                     r.FinelineNumberFreq), axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_grouped = construct_grouped_features(train_grouped, to_dummies)\n",
    "\n",
    "test_grouped = construct_grouped_features(test_grouped, to_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Plotting distribution of generated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(train_grouped.sort('TripType').TripType, \n",
    "            train_grouped.sort('TripType').MaxFlNnUptnUMr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dummy_features(data, dummy_columns):\n",
    "    data = pd.get_dummies(data, sparse=True, dummy_na=False, columns=dummy_columns)\n",
    "    return data\n",
    "\n",
    "# def minmaxscaler(feature_values):\n",
    "#     if feature_values.max() != feature_values.min():\n",
    "#         return (feature_values - feature_values.min())/(feature_values.max() - feature_values.min())\n",
    "#     else:\n",
    "#         return feature_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_excluded = ['UpcManufacturerFreq', 'UpcManufacturer', 'UpcProductFreq', 'UpcProduct', 'FinelineNumberFreq',\n",
    "                     'ScanCount', 'Upc', 'FinelineNumber'] + ['Upc_' + str(i) for i in range(1,12)]\n",
    "\n",
    "data = train_grouped.append(test_grouped)\n",
    "data = data[[col for col in data.columns if col not in features_excluded]]\n",
    "\n",
    "columns = [col for col in data.columns \n",
    "           if col not in ['DepartmentDescription', 'Weekday', 'TripType'] + features_excluded]\n",
    "# data.ix[:, columns] = data[columns].apply(minmaxscaler)\n",
    "data = construct_dummy_features(data, ['Weekday', 'DepartmentDescription'])\n",
    "\n",
    "train_dummies = data.iloc[:len(train_grouped)]\n",
    "test_dummies = data.iloc[len(train_grouped):]\n",
    "del data, test_dummies['TripType']\n",
    "print(len(test_dummies.columns), 'features constructed')\n",
    "print('Memory usage of dataframe train_dummies is %3.2f Mb' % (train_dummies.memory_usage(index=True).sum()/(1024*1024)))\n",
    "print('Memory usage of dataframe test_dummies is %3.2f Mb' % (test_dummies.memory_usage(index=True).sum()/(1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del train, test, train_grouped, test_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prior_probabilities(data):\n",
    "    prior_probabilities = np.zeros(len(types))\n",
    "    prior_probabilities += np.array([len(data[data.TripType == trip_type]) for trip_type in types])\n",
    "    prior_probabilities /= prior_probabilities.sum()\n",
    "    return prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict(train_array, train_labels, test_array, prior_probabilities=None, algorithm='rf', alpha=None, \n",
    "                plot_importance=True, feature_names=None):\n",
    "    if algorithm == 'xgb':\n",
    "        clf = xgb.XGBClassifier(n_estimators=120, max_depth=10, learning_rate=0.1, objective='multi:softprob', \n",
    "                                colsample_bytree=0.4, subsample=0.9, seed=41621)\n",
    "        clf.fit(train_array, train_labels)\n",
    "        prediction_matrix = clf.predict_proba(test_array)\n",
    "        \n",
    "        if plot_importance and feature_names:\n",
    "            with open('xgb.fmap', 'w') as outfile:\n",
    "                for i, feature_name in enumerate(feature_names):\n",
    "                    outfile.write('{0}\\t{1}\\tq\\n'.format(i, feature_name))\n",
    "            importances = clf.booster().get_fscore('xgb.fmap')\n",
    "            feature_names = list(importances.keys())\n",
    "            importances = np.array(list(importances.values()))\n",
    "            pl.title('Feature Importance')\n",
    "            sorted_indices = np.argsort(importances)[::-1]\n",
    "            for i, k in enumerate(sorted_indices):\n",
    "                print('%2d (feature %2d):' % (i, k), feature_names[k], 'Value = %.5f' % importances[k])\n",
    "            print([feature_names[k] for k in sorted_indices])\n",
    "            sns.barplot(np.arange(len(feature_names)), importances[sorted_indices], color='r')\n",
    "                        \n",
    "    elif algorithm in ['rf']:\n",
    "        if algorithm == 'rf':\n",
    "            clf = RandomForestClassifier(n_estimators=150, max_depth=20, min_samples_leaf=1, random_state=41621,\n",
    "                                         max_features=0.2, n_jobs=-1)\n",
    "        clf.fit(train_array, train_labels)\n",
    "        prediction_matrix = clf.predict_proba(test_array)\n",
    "        if plot_importance and feature_names:\n",
    "            importances = clf.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "            pl.title('Feature Importance')\n",
    "            sorted_indices = np.argsort(importances)[::-1]\n",
    "            for i, k in enumerate(sorted_indices):\n",
    "                print('%2d (feature %2d):' % (i, k), feature_names[k], 'Value = %.5f' % importances[k])\n",
    "            print([feature_names[k] for k in sorted_indices])\n",
    "\n",
    "            pl.bar(range(len(feature_names)), importances[sorted_indices], color='r',\n",
    "                   yerr=std[sorted_indices], align='center')\n",
    "            pl.xticks(range(len(feature_names)), sorted_indices)\n",
    "            pl.xlim([-1, len(feature_names) + 1])\n",
    "    \n",
    "    elif algorithm in ['logit', 'sgd']:\n",
    "        if algorithm == 'logit':\n",
    "            clf = LogisticRegressionCV(Cs=5, n_jobs=-1)\n",
    "        elif algorithm == 'sgd':\n",
    "            clf = SGDClassifier(loss='log', n_jobs=-1, alpha=10**6, class_weight='balanced')\n",
    "        clf.fit(train_array, train_labels)\n",
    "        prediction_matrix = clf.predict_proba(test_array)\n",
    "        \n",
    "    if not prior_probabilities is None:\n",
    "        coeff = 0.1\n",
    "        prediction_matrix = (prediction_matrix + [prior_probabilities*coeff]*len(prediction_matrix))/(1. + coeff)\n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_score(train_dummies, grid_space=[0], algorithm='rf', plot_importance=True, plot_errors=True):\n",
    "    if len(grid_space) > 1:\n",
    "        alphas, scores = [], []\n",
    "        min_alpha, min_score = 1., 10**5\n",
    "    \n",
    "    for alpha in grid_space:\n",
    "        if len(grid_space) > 1:\n",
    "            print('alpha =', alpha)\n",
    "        y = train_dummies.TripType.values\n",
    "        for train_indices, test_indices in sklearn.cross_validation.StratifiedShuffleSplit(y, n_iter=1, test_size=0.1):\n",
    "            data_train = train_dummies.iloc[train_indices]\n",
    "            data_test = train_dummies.iloc[test_indices]\n",
    "\n",
    "    #         prior_probabilities = get_prior_probabilities(data_train)\n",
    "            feature_names = [col for col in data_train.columns if col not in ['TripType']]\n",
    "            \n",
    "            prediction_matrix = fit_predict(data_train[feature_names].values, \n",
    "                                            data_train.TripType.values, \n",
    "                                            data_test[feature_names].values, \n",
    "                                            algorithm=algorithm, alpha=alpha, plot_importance=plot_importance, \n",
    "                                            feature_names=feature_names)\n",
    "            \n",
    "            prediction_matrix = np.delete(prediction_matrix, 7, axis=1)\n",
    "            y_true = data_test.TripType.values\n",
    "            \n",
    "            mlogloss = sklearn.metrics.log_loss(y_true, prediction_matrix) \n",
    "            \n",
    "            print('mlogloss =', mlogloss, end='\\n\\n')\n",
    "            if plot_errors:\n",
    "                pl.figure()\n",
    "                df = pd.DataFrame(y_true, columns=['TripType'])\n",
    "                df['PredictedProba'] = 0\n",
    "                for i, y in enumerate(np.unique(y_true)):\n",
    "                    df.ix[df.TripType == y, 'PredictedProba'] = prediction_matrix[y_true == y, i]\n",
    "                f, ax = pl.subplots()\n",
    "                df = df.sort('TripType')\n",
    "                sns.boxplot(df.TripType, df.PredictedProba, ax=ax)\n",
    "                f.savefig('proba_errors.png')\n",
    "            \n",
    "            if len(grid_space) > 1:\n",
    "                alphas.append(alpha)\n",
    "                scores.append(mlogloss)\n",
    "                if mlogloss <= min_score:\n",
    "                    min_score = mlogloss\n",
    "                    min_alpha = alpha\n",
    "    if len(grid_space) > 1:\n",
    "        return alphas, scores, min_alpha, min_score\n",
    "    else:\n",
    "        return mlogloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dump = train_dummies\n",
    "# test_dump = test_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_selection = ['ModeFinelineNumber', 'VisitNumber', 'MedianFinelineNumberFreq', 'ModeUpcProduct', 'MedianFlNnUptnUMr', 'SumSnCtFlNFreq', 'MedianUpcManufacturerFreq', 'SumSnCtUPtFreq', 'SumSnCtUMrFreq', 'MedianUpcProductFreq', 'StdSnCtFlNFreq', 'MeanDiffUMrUpt', 'MaxFlNnUptnUMr', 'ModeUpcManufacturer', 'StdRelUMrUpt', 'StdSnCtUMrFreq', 'StdSnCtUPtFreq', 'DepartmentFreq', 'Upc_1_value0', 'LogNumberOfPurchased', 'Upc_1_value6', 'Upc_1_value8', 'Upc_1_value7', 'MaxScanCount', 'UMrDummy_81131', 'UMrDummy_78742', 'UMrDummy_0', 'Upc_1_value3', 'UMrDummy_37000', 'UMrDummy_5388', 'UMrDummy_49000', 'Weekday_Saturday', 'Weekday_Sunday', 'UMrDummy_28400', 'Weekday_Monday', 'Weekday_Tuesday', 'Weekday_Friday', 'UMrDummy_35000', 'Weekday_Wednesday', 'Weekday_Thursday', 'MedianScanCount', 'UMrDummy_34000', 'FlNDummy_135', 'UMrDummy_33383', 'Upc_1_value2', 'LogNumberOfReturned', 'FlNDummy_5501', 'UPtDummy_0', 'UPtDummy_4011', 'UMrDummy_44000', 'UMrDummy_38000', 'FlNDummy_0', 'DepartmentDescription_DSD GROCERY', 'FlNDummy_808', 'FlNDummy_1508', 'FlNDummy_115', 'DepartmentDescription_IMPULSE MERCHANDISE', 'FlNDummy_9546', 'UPtDummy_62097', 'FlNDummy_100', 'DepartmentDescription_PERSONAL CARE', 'DepartmentDescription_PRODUCE', 'DepartmentDescription_PHARMACY OTC', 'FlNDummy_203', 'FlNDummy_202', 'FlNDummy_3004', 'FlNDummy_801', 'FlNDummy_4010', 'FlNDummy_110', 'FlNDummy_1407', 'Upc_1_value4', 'FlNDummy_4606', 'FlNDummy_10000', 'DepartmentDescription_DAIRY', 'FlNDummy_3467', 'FlNDummy_5017', 'FlNDummy_3601', 'FlNDummy_5620', 'DepartmentDescription_GROCERY DRY GOODS', 'FlNDummy_8101', 'DepartmentDescription_FROZEN FOODS', 'DepartmentDescription_BEAUTY', 'DepartmentDescription_HOUSEHOLD CHEMICALS/SUPP', 'FlNDummy_3600', 'FlNDummy_7010', 'FlNDummy_3555', 'FlNDummy_9100', 'DepartmentDescription_MENS WEAR', 'UPtDummy_35186', 'FlNDummy_3702', 'FlNDummy_7955', 'FlNDummy_9570', 'UPtDummy_35187', 'DepartmentDescription_CANDY, TOBACCO, COOKIES', 'FlNDummy_4624', 'UPtDummy_44', 'FlNDummy_9101', 'DepartmentDescription_CELEBRATION', 'DepartmentDescription_HOUSEHOLD PAPER GOODS', 'UPtDummy_20027', 'UPtDummy_4087', 'FlNDummy_3120', 'UPtDummy_4046', 'DepartmentDescription_COMM BREAD', 'DepartmentDescription_PETS AND SUPPLIES', 'DepartmentDescription_BAKERY', 'UPtDummy_7862', 'DepartmentDescription_SERVICE DELI', 'DepartmentDescription_HOME MANAGEMENT', 'DepartmentDescription_COOK AND DINE', 'DepartmentDescription_OFFICE SUPPLIES', 'DepartmentDescription_FINANCIAL SERVICES', 'DepartmentDescription_HARDWARE', 'DepartmentDescription_AUTOMOTIVE', 'UPtDummy_71457', 'UPtDummy_4062', 'UPtDummy_71461', 'DepartmentDescription_LADIESWEAR', 'DepartmentDescription_INFANT CONSUMABLE HARDLINES', 'DepartmentDescription_SPORTING GOODS', 'DepartmentDescription_TOYS', 'DepartmentDescription_LIQUOR,WINE,BEER', 'DepartmentDescription_LAWN AND GARDEN', 'DepartmentDescription_SHOES']\n",
    "# train_dummies = train_dump[feature_selection + ['TripType']]\n",
    "# test_dummies = test_dump[feature_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dummies = train_dump\n",
    "# test_dummies = test_dump\n",
    "# del train_dump, test_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [col for col in test_dummies.columns \n",
    "           if not np.any([col.startswith(s) for s in ['FlNDummy','UMrDummy','UPtDummy','DepartmentDescription', 'Weekday', 'Upc_']])]\n",
    "corr = train_dummies[columns].append(test_dummies[columns]).corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, square=True, robust=True, linewidths=0.2, annot=False)\n",
    "del corr, mask, cmap, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "algorithms = ['rf', 'xgb', 'logit', 'sgd']\n",
    "algorithm = 'xgb'\n",
    "\n",
    "validation = True\n",
    "\n",
    "if validation:\n",
    "    \n",
    "    compute_optimal = False\n",
    "    if compute_optimal:\n",
    "        grid_space = np.arange(120, 201, 30)\n",
    "        alphas, scores, min_alpha, min_score = train_and_score(train_dummies, grid_space=grid_space,\n",
    "                                                               algorithm=algorithm, plot_importance=False)\n",
    "        print('MIN ALPHA =', min_alpha, 'MIN_MLOGLOSS =', min_score)\n",
    "        pl.figure()\n",
    "        pl.plot(alphas, scores, 'r.--')\n",
    "    else:\n",
    "        mlogloss = train_and_score(train_dummies, algorithm=algorithm)\n",
    "    \n",
    "elif not validation:\n",
    "#     prior_probabilities = get_prior_probabilities(train)\n",
    "    prediction_matrix = fit_predict(train_dummies[[col for col in train_dummies.columns \n",
    "                                                   if col not in ['TripType']]].values, \n",
    "                                    train_dummies.TripType.values, test_dummies.values, algorithm=algorithm)\n",
    "    \n",
    "    prediction_df = pd.DataFrame(prediction_matrix, columns=['TripType_' + str(i) for i in types])\n",
    "    prediction_df['VisitNumber'] = test_dummies.VisitNumber.values\n",
    "    \n",
    "    prediction_df = prediction_df[['VisitNumber'] + [x for x in prediction_df.columns if x != 'VisitNumber']]\n",
    "    del prediction_matrix\n",
    "    \n",
    "    filename = 'prediction.csv'\n",
    "    prediction_df.to_csv(filename, index=False)\n",
    "    print(filename, 'was created')\n",
    "    del prediction_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
